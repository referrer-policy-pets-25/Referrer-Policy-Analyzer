{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Circumvention\n",
    "\n",
    "Check RP circumvention on three vetors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "from pandarallel import pandarallel\n",
    "import os\n",
    "\n",
    "from helpers.utils import (\n",
    "    check_urlparse,\n",
    "    check_circumvention,\n",
    "    validate_referrer,\n",
    "    create_summary,\n",
    "    mark_fp_rows,\n",
    "    clean_column_detail\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 2024-01-24\n",
      "Type: AMS\n"
     ]
    }
   ],
   "source": [
    "#folder_path = '../output/data_leaks/2025-02-08_noAct_leaks.parquet'\n",
    "# folder_path = '../output/data_leaks/2024-01-24_SF_leaks.parquet'\n",
    "# folder_path = '../output/data_leaks/2024-01-24_SG_leaks.parquet'\n",
    "folder_path = '../output/data_leaks/2024-01-24_AMS_leaks.parquet'\n",
    "\n",
    "df = pd.read_parquet(folder_path)\n",
    "\n",
    "\n",
    "# parquet_file = './output/2025-02-08_noAct_extended_leaks_raw.parquet'\n",
    "# parquet_file = './output/2025-02-08_optIn_extended_leaks_raw.parquet'\n",
    "# parquet_file = './output/2025-02-08_optOut_extended_leaks_raw.parquet'\n",
    "\n",
    "import re\n",
    "match = re.search(r'(\\d{4}-\\d{2}-\\d{2})_([^_]+)', folder_path)\n",
    "if match:\n",
    "    extract_date, extract_type = match.groups()\n",
    "    print(\"Date:\", extract_date)\n",
    "    print(\"Type:\", extract_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 16 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "pandarallel.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6433459, 22)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the data\n",
    "df = df[df['failed_visit'] == False]\n",
    "df = df[df['req_url'].str.startswith('http')]\n",
    "df = df[df.final_url != \"about:blank\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6336437, 22)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the URL components for referrer vector\n",
    "\n",
    "df[['ref_final_found', 'ref_full', 'ref_netloc', 'ref_hostname', 'ref_path', \n",
    "                'ref_params', 'ref_query', 'ref_fragments']] = df.parallel_apply(\n",
    "                    lambda row: check_urlparse(row, \"ref\"), axis=1, result_type=\"expand\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for circumvention for referrer vector\n",
    "df[['ref_flag', 'ref_frag_found']] = df.parallel_apply(\n",
    "    lambda row: check_circumvention(row, \"ref\"), axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Omitting false positive in referrer vector\n",
    "df = validate_referrer(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ref_summary'] = df.parallel_apply(lambda row: create_summary(row, 'ref'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the URL components for request URL vector\n",
    "df[['url_final_found', 'url_full', 'url_netloc', 'url_hostname', 'url_path', \n",
    "                   'url_params', 'url_query', 'url_fragments']] = df.parallel_apply(\n",
    "                       lambda row: check_urlparse(row, \"url\"), axis=1, result_type=\"expand\")\n",
    "\n",
    "# Checking for circumvention for request URL vector\n",
    "df[['url_flag', 'url_frag_found']] = df.parallel_apply(\n",
    "    lambda row: check_circumvention(row, \"url\"), axis=1, result_type='expand')\n",
    "\n",
    "# Omitting false positive in request URL vector\n",
    "df = mark_fp_rows(df, \"url\")\n",
    "\n",
    "# Creating the summary for request URL vector\n",
    "df['url_summary'] = df.parallel_apply(lambda row: create_summary(row, 'url'), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the URL components for post vector\n",
    "df[['post_final_found', 'post_full', 'post_netloc', 'post_hostname', 'post_path', \n",
    "                   'post_params', 'post_query', 'post_fragments']] = df.parallel_apply(\n",
    "                       lambda row: check_urlparse(row, \"post\"), axis=1, result_type=\"expand\")\n",
    "\n",
    "# Checking for circumvention for post vector\n",
    "df[['post_flag', 'post_frag_found']] = df.parallel_apply(\n",
    "    lambda row: check_circumvention(row, \"post\"), axis=1, result_type='expand')\n",
    "\n",
    "# Omitting false positive in post vector\n",
    "df = mark_fp_rows(df, \"post\")\n",
    "\n",
    "# Creating the summary for post vector\n",
    "df['post_summary'] = df.parallel_apply(lambda row: create_summary(row, 'post'), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = clean_column_detail(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"../output/circum\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(\"../output/circum/\"+extract_date+\"_\"+extract_type+'_circum.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
